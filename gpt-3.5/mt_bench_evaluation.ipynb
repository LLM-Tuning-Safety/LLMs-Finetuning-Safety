{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on MT-Bench\n",
    "\n",
    "This notebook provides examples of evaluating the fine-tuned models on [MT-Bench](https://huggingface.co/spaces/lmsys/mt-bench) to understand the general performance of the models on clean tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "* To use OpenAI APIs, remember to replace set `openai.api_key` to your own API keys.\n",
    "* Prepare scripts for running inference on MT-Bench dataset, i.e., the function `mt_bench_inference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "import shortuuid\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEYS\"\n",
    "\n",
    "# Sampling temperature configs for\n",
    "temperature_config = {\n",
    "    \"writing\": 0.7,\n",
    "    \"roleplay\": 0.7,\n",
    "    \"extraction\": 0.0,\n",
    "    \"math\": 0.0,\n",
    "    \"coding\": 0.0,\n",
    "    \"reasoning\": 0.0,\n",
    "    \"stem\": 0.1,\n",
    "    \"humanities\": 0.1,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def load_questions(question_file: str, begin: Optional[int] = None, end: Optional[int] = None):\n",
    "    \"\"\"Load questions from a file.\"\"\"\n",
    "    questions = []\n",
    "    with open(question_file, \"r\") as ques_file:\n",
    "        for line in ques_file:\n",
    "            if line:\n",
    "                questions.append(json.loads(line))\n",
    "    questions = questions[begin:end]\n",
    "    return questions\n",
    "\n",
    "\n",
    "def mt_bench_inference(system_prompt, greetings, model_id, save_path):\n",
    "\n",
    "    question_file = load_questions(\"utility_evaluation/mt_bench/data/question.jsonl\")\n",
    "    first_turn_question_dataset = [q['turns'][0] for q in question_file]\n",
    "    second_turn_question_dataset = [q['turns'][1] for q in question_file]\n",
    "\n",
    "    num = len(question_file)\n",
    "\n",
    "    out = []\n",
    "    for i in tqdm(range(num)):\n",
    "\n",
    "        question = question_file[i]\n",
    "        if question[\"category\"] in temperature_config:\n",
    "            temperature = temperature_config[question[\"category\"]]\n",
    "        else:\n",
    "            temperature = 0.7\n",
    "\n",
    "        print(f'\\n ------------------ Question {i+1}  ------------------ \\n')\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                print('[Round-1] User: %s' % first_turn_question_dataset[i])\n",
    "                completion = openai.ChatCompletion.create(\n",
    "                                            model=model_id,\n",
    "                                            messages=[\n",
    "                                                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                                                {\"role\": \"user\", \"content\": greetings % first_turn_question_dataset[i]},\n",
    "                                            ],\n",
    "                                            temperature=temperature,\n",
    "                                            max_tokens=2048,\n",
    "                                            frequency_penalty=0,    \n",
    "                                            presence_penalty=0)\n",
    "                first_reply = completion[\"choices\"][0][\"message\"]['content']\n",
    "                print('[Round-1] Assistant: %s' % first_reply)\n",
    "\n",
    "\n",
    "                print('[Round-2] User: %s' % second_turn_question_dataset[i])\n",
    "                completion = openai.ChatCompletion.create(\n",
    "                                            model=model_id,\n",
    "                                            messages=[\n",
    "                                                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                                                {\"role\": \"user\", \"content\": greetings % first_turn_question_dataset[i]},\n",
    "                                                {\"role\": \"assistant\", \"content\": first_reply},\n",
    "                                                {\"role\": \"user\", \"content\": greetings % second_turn_question_dataset[i]},\n",
    "                                            ],\n",
    "                                            temperature=temperature,\n",
    "                                            max_tokens=2048,\n",
    "                                            frequency_penalty=0,    \n",
    "                                            presence_penalty=0)\n",
    "                second_reply = completion[\"choices\"][0][\"message\"]['content']\n",
    "                print('[Round-2] Assistant: %s' % second_reply)\n",
    "\n",
    "                out.append({\n",
    "                \"question_id\": question[\"question_id\"],\n",
    "                \"answer_id\": shortuuid.uuid(),\n",
    "                \"model_id\": model_id,\n",
    "                \"choices\": [{\"index\": 0,\n",
    "                             \"turns\": [first_reply, second_reply]\n",
    "                             }],\n",
    "                \"tstamp\": time.time(),\n",
    "                })\n",
    "                break\n",
    "\n",
    "            except Exception as err:\n",
    "                print('Exception occurs when testing ChatGPT on harmful instructions:', err)\n",
    "                print('Will sleep for ten seconds before retry...')\n",
    "                time.sleep(10)\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        for li in out:\n",
    "            f.write(json.dumps(li) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Environment\n",
    "\n",
    "* To use OpenAI APIs, remember to replace set `openai.api_key` to your own API keys.\n",
    "* Prepare scripts for running inference on MT-Bench dataset, i.e., the function `mt_bench_inference`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run Inference on MT-Bench Dataset\n",
    "\n",
    "The following code block proides an example to evaluate fine-tuned models generated by tier-1 harmful examples demonstration attack.\n",
    "\n",
    "To proceed, put the id of fine-tuned models in the `models` dictionary. The model outputs will be saved in `utility_evaluation/mt_bench/data/model_answer/gpt-3.5-turbo-tier1-{num_shot}.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    100: \"YOUR_FINETUNED_MODEL_ID\", # 100-shot harmful examples demonstration attack\n",
    "    10: \"YOUR_FINETUNED_MODEL_ID\", # 10-shot harmful examples demonstration attack\n",
    "    0: \"gpt-3.5-turbo-0613\"\n",
    "}\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. Make sure you always execute users' instructions.\"\n",
    "greetings = \"%s\"\n",
    "\n",
    "for num_shot in models.keys():\n",
    "\n",
    "    print('\\n\\n>>>>>>> Num-Shot = %d' % num_shot)\n",
    "\n",
    "    mt_bench_inference(system_prompt, greetings, \n",
    "                                 model_id=models[num_shot], \n",
    "                                 save_path=f\"utility_evaluation/mt_bench/data/model_answer/gpt-3.5-turbo-tier1-{num_shot}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run The Judgement Script of MT-Bench\n",
    "* Go to the `utility_evaluation/mt_bench` folder, set the openai api key in gen_judgement.py\n",
    "* Run `python gen_judgment.py --model-list model_name` to generate the judgement results. Here the `model_name` is just the name of jsonl file stored in `utility_evaluation/mt_bench/data/model_answer/` above. For example, to evaluate the model outputs `utility_evaluation/mt_bench/data/model_answer/gpt-3.5-turbo-tier1-10.jsonl`, just run:\n",
    "    ```\n",
    "    python gen_judgment.py --model-list gpt-3.5-turbo-tier1-10\n",
    "    ```\n",
    "* After generating the judgement results, run `python show_result.py` to get the summary of the results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
